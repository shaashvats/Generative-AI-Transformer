{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Binary cross-entropy loss function\n",
    "def compute_cost(y, y_pred):\n",
    "    m = y.shape[0]  # Number of examples\n",
    "    cost = -(1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    return cost\n",
    "\n",
    "# Gradient descent function\n",
    "def gradient_descent(X, y, weights, bias, learning_rate, epochs):\n",
    "    m = X.shape[0]  # Number of examples\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Forward propagation\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        # Compute the cost\n",
    "        cost = compute_cost(y, y_pred)\n",
    "\n",
    "        # Backward propagation (gradient computation)\n",
    "        dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "        db = (1/m) * np.sum(y_pred - y)\n",
    "\n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    return weights, bias, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Gate\n",
    "Made using a logistic regression i.e. neural net with one neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [[7.41918007]\n",
      " [7.41918041]]\n",
      "Trained bias: -11.301366033410076\n",
      "\n",
      "Predictions:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Input and output for AND gate\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Initialize weights and bias\n",
    "#weights = np.zeros((X.shape[1], 1))  # 2x1 for 2 input features\n",
    "weights = np.random.rand(X.shape[1], 1)\n",
    "bias = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Train the logistic regression model\n",
    "weights, bias, cost = gradient_descent(X, y, weights, bias, learning_rate, epochs)\n",
    "\n",
    "print(\"Trained weights:\", weights)\n",
    "print(\"Trained bias:\", bias)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sigmoid(np.dot(X, weights) + bias)\n",
    "y_pred = np.round(y_pred)  # Round to get 0 or 1\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Gate \n",
    "Using a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [[2.49330776e-16]\n",
      " [2.48742219e-16]]\n",
      "Trained bias: -3.31850112596023e-16\n",
      "\n",
      "Predictions:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Input and output for xor gate\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights and bias\n",
    "#weights = np.zeros((X.shape[1], 1))  # 2x1 for 2 input features\n",
    "weights = np.random.rand(X.shape[1], 1)\n",
    "bias = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Train the logistic regression model\n",
    "weights, bias, cost = gradient_descent(X, y, weights, bias, learning_rate, epochs)\n",
    "\n",
    "print(\"Trained weights:\", weights)\n",
    "print(\"Trained bias:\", bias)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sigmoid(np.dot(X, weights) + bias)\n",
    "y_pred = np.round(y_pred)  # Round to get 0 or 1\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issue with using a perceptron for an XOR gate:**\n",
    "- There is no linear function that can make a linear combination of (1,0) and (0,1) greater than that of (0,0) and (1,1) simultaneously.\n",
    "- Hence there is a need to build a hidden layer\n",
    "- Intuitively, I think that the hidden layer will need only two neurons so I will start by trying that\n",
    "\n",
    "**Using a hidden layer for an XOR gate**\n",
    "\n",
    "- The first task is improving the functions to handle forward, backprop for multi layer\n",
    "- Then updating the model training data for an XOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing better functions\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Binary cross-entropy loss function\n",
    "def compute_cost(y, y_pred):\n",
    "    m = y.shape[0]  # Number of examples\n",
    "    cost = -(1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    return cost\n",
    "\n",
    "# Neural network training function\n",
    "def train_neural_network(X, y, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    input_neurons = X.shape[1]\n",
    "    m = X.shape[0]  # Number of examples\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.rand(input_neurons, hidden_neurons)  # Input to hidden layer\n",
    "    b1 = np.zeros((1, hidden_neurons))\n",
    "    W2 = np.random.rand(hidden_neurons, output_neurons)  # Hidden to output layer\n",
    "    b2 = np.zeros((1, output_neurons))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Forward propagation\n",
    "        Z1 = np.dot(X, W1) + b1\n",
    "        A1 = sigmoid(Z1)  # Activation of hidden layer\n",
    "        Z2 = np.dot(A1, W2) + b2\n",
    "        A2 = sigmoid(Z2)  # Activation of output layer (predictions)\n",
    "\n",
    "        # Compute the cost\n",
    "        cost = compute_cost(y, A2)\n",
    "\n",
    "        # Backward propagation\n",
    "        dZ2 = A2 - y\n",
    "        dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = np.dot(dZ2, W2.T)\n",
    "        dZ1 = dA1 * sigmoid_derivative(Z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights and biases\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "\n",
    "    return W1, b1, W2, b2, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights and biases:\n",
      "W1: [[4.57608671 6.51164718]\n",
      " [4.578482   6.52534109]]\n",
      "b1: [[-7.00486476 -2.86347719]]\n",
      "W2: [[-10.32773912]\n",
      " [  9.61540107]]\n",
      "b2: [[-4.3991656]]\n",
      "\n",
      "Predictions:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Now updating model for XOR\n",
    "\n",
    "# XOR gate input and output\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0], [1], [1], [0]])  # Output for XOR gate\n",
    "\n",
    "# Set hyperparameters\n",
    "hidden_neurons = 2  # Number of neurons in the hidden layer\n",
    "output_neurons = 1  # Number of output neurons\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Train the neural network\n",
    "W1, b1, W2, b2, cost = train_neural_network(X, y, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "\n",
    "print(\"Trained weights and biases:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"b2:\", b2)\n",
    "\n",
    "# Make predictions\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "predictions = np.round(A2)  # Round to get binary output\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XOR Success**\n",
    "- This has been successful. A hidden layer with two neurons successfully makes an XOR gate\n",
    "- Now onto the Full Adder\n",
    "### Full Adder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights and biases:\n",
      "W1: [[4.36184396 2.08160792 2.3240974  6.60643639]\n",
      " [4.37140089 2.13188855 2.27446036 6.60566585]\n",
      " [4.39303902 2.51765735 1.96547882 6.61061256]]\n",
      "b1: [[-1.88606008 -5.5973929  -5.50020541 -9.69918861]]\n",
      "W2: [[  8.19534238   0.93871435]\n",
      " [  6.09788206   4.25425231]\n",
      " [  7.09584195   3.50040505]\n",
      " [-11.27013142   8.85934498]]\n",
      "b2: [[-3.9591962  -6.44376329]]\n",
      "\n",
      "Predictions:\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Full Adder input and output\n",
    "X = np.array([[0, 0, 0],\n",
    "              [0, 0, 1],\n",
    "              [0, 1, 0],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 0],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 0],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "y = np.array([[0, 0],  # S = 0, Cout = 0\n",
    "              [1, 0],  # S = 1, Cout = 0\n",
    "              [1, 0],  # S = 1, Cout = 0\n",
    "              [0, 1],  # S = 0, Cout = 1\n",
    "              [1, 0],  # S = 1, Cout = 0\n",
    "              [0, 1],  # S = 0, Cout = 1\n",
    "              [0, 1],  # S = 0, Cout = 1\n",
    "              [1, 1]]) # S = 1, Cout = 1\n",
    "\n",
    "# Set hyperparameters\n",
    "hidden_neurons = 4  # Number of neurons in the hidden layer\n",
    "output_neurons = 2  # Number of output neurons (S and Cout)\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Train the neural network\n",
    "W1, b1, W2, b2, cost = train_neural_network(X, y, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "\n",
    "print(\"Trained weights and biases:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"b1:\", b1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"b2:\", b2)\n",
    "\n",
    "# Make predictions\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "predictions = np.round(A2)  # Round to get binary output\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "\n",
    "y=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions from Full Adder Instance**\n",
    "- Two hidden neurons are insufficient\n",
    "- Three are sufficient\n",
    "\n",
    "### Ripple Carry Adder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111  +  11111  = \n",
      "111110\n"
     ]
    }
   ],
   "source": [
    "# n bit adder\n",
    "n = 5\n",
    "a = \"11111\"\n",
    "b = \"11111\"\n",
    "sum = \"\"\n",
    "ins_for_net = np.zeros((1, 3), dtype=int)\n",
    "\n",
    "for i in range(0,n):\n",
    "    ins_for_net[0,0] = a[n-i-1]   # sets the first value in input as a[] \n",
    "    ins_for_net[0,1] = b[n-i-1]   # sets the second value in input as b[] \n",
    "    Z1 = np.dot(ins_for_net, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    predictions = np.round(A2)\n",
    "    ins_for_net[0,2] = predictions[0,1]\n",
    "    sum = str(int(predictions[0,0])) + sum\n",
    "    #print(predictions)\n",
    "\n",
    "sum = str(int(predictions[0,1]))+sum\n",
    "print(a, \" + \",b, \" = \")\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- Neural nets work have multiple layers of multiple logistic regressions\n",
    "- They make use of non-linearities of logistic regressions to achieve interesting effects\n",
    "- Back propogogation is done simultaneously to all layers, hence it makes sense to seed coefficients randomly or the network can behave \"symmetrically\"\n",
    "- I didnt make the computer \"learn\" a ripple carry adder as it made more sense to use the full adder it had already learnt and cascade it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
