{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class CharacterTransformer(tf.keras.Model):  # Changed to inherit from tf.keras.Model\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 d_model=256,\n",
    "                 n_heads=8,\n",
    "                 ff_dim=512,\n",
    "                 max_seq_length=100,\n",
    "                 dropout_rate=0.1):\n",
    "        super().__init__()  # Added super().__init__() call\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Initialize embeddings and positional encodings\n",
    "        self.char_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self._positional_encoding()\n",
    "        \n",
    "        # Initialize transformer blocks\n",
    "        self.attention_block = self.build_attention_block()\n",
    "        self.ff_block = self.build_feedforward_block()\n",
    "        \n",
    "        # Output layers\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # Add dropout layers\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def _positional_encoding(self):\n",
    "        pos = np.arange(self.max_seq_length)[:, np.newaxis]\n",
    "        i = np.arange(self.d_model)[np.newaxis, :]\n",
    "        angle = pos / np.power(10000, (2 * (i//2)) / self.d_model)\n",
    "        \n",
    "        pos_encoding = np.zeros_like(angle)\n",
    "        pos_encoding[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "        \n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def build_attention_block(self):\n",
    "        return tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.n_heads,\n",
    "            key_dim=self.d_model // self.n_heads,\n",
    "            dropout=self.dropout_rate\n",
    "        )\n",
    "    \n",
    "    def build_feedforward_block(self):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(self.dropout_rate),\n",
    "            tf.keras.layers.Dense(self.d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self, x, training=False):  # Changed from __call__ to call\n",
    "        # Get sequence length\n",
    "        seq_length = tf.shape(x)[1]\n",
    "        \n",
    "        # Create embeddings\n",
    "        x = self.char_embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x += self.pos_encoding[:seq_length, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Self-attention block\n",
    "        attention_output = self.attention_block(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "        x = self.layer_norm1(x + attention_output)\n",
    "        \n",
    "        # Feedforward block\n",
    "        ff_output = self.ff_block(x, training=training)\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.final_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def prepare_data(text, seq_length):\n",
    "    # Create character vocabulary\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "    idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "    \n",
    "    # Convert text to indices\n",
    "    text_as_int = [char_to_idx[char] for char in text]\n",
    "    \n",
    "    # Create input sequences and target values\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    \n",
    "    for i in range(0, len(text_as_int) - seq_length):\n",
    "        sequences.append(text_as_int[i:i + seq_length])\n",
    "        next_chars.append(text_as_int[i + seq_length])\n",
    "    \n",
    "    x = np.array(sequences)\n",
    "    y = np.array(next_chars)\n",
    "    \n",
    "    return x, y, char_to_idx, idx_to_char\n",
    "\n",
    "def train_model(model, x, y, epochs=10, batch_size=64):\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    # Define optimizer and loss\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                logits = model(batch_x, training=True)\n",
    "                # Reshape logits and compute loss\n",
    "                batch_loss = loss_fn(batch_y, logits[:, -1, :])\n",
    "            \n",
    "            # Compute gradients and apply updates\n",
    "            gradients = tape.gradient(batch_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def generate_text(model, start_string, char_to_idx, idx_to_char, num_chars=1000, temperature=1.0):\n",
    "    # Convert start string to indices\n",
    "    input_eval = [char_to_idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    for _ in range(num_chars):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = predictions[:, -1, :] / temperature\n",
    "        \n",
    "        # Sample from the predicted distribution\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # Append prediction to generated text\n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "        \n",
    "        # Update input evaluation\n",
    "        input_eval = tf.concat([input_eval[:, 1:], \n",
    "                              tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0332\n",
      "Epoch 2, Loss: 2.0738\n",
      "Epoch 3, Loss: 2.0846\n",
      "Epoch 4, Loss: 2.0976\n",
      "Epoch 5, Loss: 2.1084\n",
      "Epoch 6, Loss: 2.1103\n",
      "Epoch 7, Loss: 2.1027\n",
      "Epoch 8, Loss: 2.0834\n",
      "Epoch 9, Loss: 2.1069\n",
      "Epoch 10, Loss: 2.0972\n"
     ]
    }
   ],
   "source": [
    "# Load your Shakespeare text file\n",
    "with open('training_data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Set parameters\n",
    "seq_length = 100\n",
    "vocab_size = len(set(text))\n",
    "\n",
    "# Prepare data\n",
    "x, y, char_to_idx, idx_to_char = prepare_data(text, seq_length)\n",
    "\n",
    "# Create and train model\n",
    "model = CharacterTransformer(vocab_size=vocab_size)\n",
    "train_model(model, x, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whyos domanfinae, t l;\n",
      "rothereansly: r aa,\n",
      "r\n",
      "t k\n",
      ":\n",
      "srnevey m,\n",
      "si e\n",
      "te\n",
      "-'n on\n",
      "\n",
      "\n",
      ",\n",
      "d,\n",
      "e\n",
      "ow--paoIiN:\n",
      "e\n",
      "t.\n",
      "igiso s\n",
      ",\n",
      "\n",
      "s\n",
      "ted ! e\n",
      ", llet.s\n",
      "tusinothneethewaw,\n",
      "lestotoforendgend; ao?uthau\n",
      "\n",
      "baicheure.\n",
      "u:\n",
      ":\n",
      "!,\n",
      ",\n",
      "\n",
      "!\n",
      "sothactaheeatahIfeHIng f?\n",
      "IahoeathevON:\n",
      " .\n",
      "s\n",
      "lionopeikno t \n",
      "o:\n",
      "IO\n",
      "t inf he r l tyorido lolnon: ha\n",
      ".\n",
      ".\n",
      ",\n",
      "\n",
      " \n",
      "Tag.\n",
      "isiealethaiaN:\n",
      "tl i!\n",
      "dinion: aneremewobasswaivel l\n",
      "sh\n",
      "\n",
      "dt icembd t io,ofeibag,\n",
      "s\n",
      "laxesuyethathaulenedsmofen,\n",
      "\n",
      "r, p,\n",
      "uselon is\n",
      "ak\n",
      "-\n",
      "gs\n",
      ",\n",
      "dotofoigsage:'t\n",
      "\n",
      "!\n",
      "g\n",
      "\n",
      ".\n",
      "s.\n",
      "e\n",
      "r m o ok\n",
      "t,\n",
      "as\n",
      "a ,\n",
      "\n",
      "s\n",
      "sa,\n",
      "! d\n",
      "\n",
      "e, aengonetud end h sddODO, cho io a:\n",
      "\n",
      "\n",
      ":\n",
      "wige reifet yeeat.\n",
      "\n",
      "e,\n",
      "\n",
      "d\n",
      "t\n",
      "s n\n",
      "tintnnthaaacerolibe;\n",
      "e\n",
      "ak't ssccouseang\n",
      "; \n",
      " h!\n",
      "n.\n",
      "ngsusheeasiasofag\n",
      ",\n",
      "fabrias e: eseantpof ceaheeaA'bsealo le\n",
      " m \n",
      "r;\n",
      "\n",
      "sh a,\n",
      "n,\n",
      "\n",
      "mowslinenout t tsuwve r,\n",
      "'s,\n",
      "d\n",
      "-g,\n",
      "t bf;IONIOre\n",
      "\n",
      "r\n",
      "s?\n",
      "ion st, w,\n",
      "ss; hepubt led \n",
      "\n",
      "\n",
      "xcet to uglarerooue\n",
      "co l eo'l dasoeihfoaesan:\n",
      "d\n",
      "stheleaela:'d\n",
      "d,\n",
      "il I ,\n",
      "\n",
      "ow\n",
      "\n",
      "t.\n",
      "re.\n",
      "\n",
      "\n",
      "l co'letowngt t ds\n",
      "\n",
      "'\n",
      "\n",
      "\n",
      "s\n",
      ",\n",
      "\n",
      "\n",
      "ge?oyon:\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "h,\n",
      "t:\n",
      "\n",
      ",\n",
      "n;\n",
      "\n",
      "'\n",
      "\n",
      ";\n",
      "\n",
      "s\n",
      "\n",
      ",\n",
      "ch ,\n",
      "\n",
      "f e.\n",
      "\n",
      "y\n",
      "\n",
      "e\n",
      "ly\n",
      ":\n",
      "a d\n",
      "us \n",
      ",\n",
      "ng\n",
      "\n",
      "\n",
      "ce\n",
      "t kis m\n",
      "den\n"
     ]
    }
   ],
   "source": [
    "# Generate new text\n",
    "generated_text = generate_text(model, \n",
    "                             start_string=\"Why\", \n",
    "                             char_to_idx=char_to_idx, \n",
    "                             idx_to_char=idx_to_char)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to character_transformer_weights.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# Corrected filepath with the required `.weights.h5` suffix\n",
    "file_path = \"character_transformer_weights.weights.h5\"\n",
    "\n",
    "# Save the model's weights\n",
    "model.save_weights(file_path)\n",
    "print(f\"Model weights saved to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
